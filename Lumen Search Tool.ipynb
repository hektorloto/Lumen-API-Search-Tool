{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a69e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from csv import writer\n",
    "from os.path import exists\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa7e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentation at : https://github.com/berkmancenter/lumendatabase/wiki/Lumen-API-documentation\n",
    "# Read the API Terms of Use at : https://lumendatabase.org/pages/api_terms\n",
    "\n",
    "destFile = \"output.csv\"\n",
    "infringing = \"infringing_urls.csv\"\n",
    "\n",
    "load_dotenv()\n",
    "# environment variables\n",
    "APIkey = os.getenv(\"API_KEY\")\n",
    "userAgent = os.getenv(\"USER_AGENT\")\n",
    "\n",
    "typeSearch = \"term\"\n",
    "endpoint = \"https://lumendatabase.org/notices/search.json?\"\n",
    "headers = {'User-Agent': userAgent}\n",
    "fieldnames = ['id',\n",
    "            'type',\n",
    "            'date_received',\n",
    "            'sender_name',\n",
    "            'jurisdictions',\n",
    "            'copyrighted_urls',\n",
    "            'infringing_urls',\n",
    "            'searched_url',\n",
    "            'score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Search entry or MODE 1\n",
    "init_url = [\"https://newsworldinfo.over-blog.com/\"]\n",
    "\n",
    "# URLs or other words to skip.\n",
    "to_skip = [\"No URL submitted\"]\n",
    "\n",
    "# Words to search for when to recognize a blogging platform (MODE 3, SAVE_ONLY_BAD_URLS)\n",
    "bad_words = [\"blogspot\", \n",
    "           \"issuu\", \n",
    "           \"livejournal\",\n",
    "           \"weebly\",\n",
    "           \"wordpress\",\n",
    "           \"tumblr\",\n",
    "           \"over-blog\",\n",
    "            \"food.blog\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODES (default = 1)\n",
    "\n",
    "# 1 : Search the init_url list\n",
    "# 2 : Search the infringing_urls list contained in LumenResult_0421.csv\n",
    "# 3 : Search the urls found using bad_words\n",
    "# 4 : Save the infringing urls found in a CSV File\n",
    "\n",
    "GET_URLS = 1\n",
    "\n",
    "# SAVE_ONLY_BAD_URLS\n",
    "\n",
    "# True : Only saves results that include one of the bad_words in the infringing_url field (decrease noise)\n",
    "# False: Saves all results without discriminating (increase noise)\n",
    "\n",
    "SAVE_ONLY_BAD_URLS = True\n",
    "\n",
    "def contains_bad_word(url):\n",
    "    for u in bad_words:\n",
    "        if u in url:\n",
    "            return True,u\n",
    "    return False,None\n",
    "\n",
    "def get_infringing_urls():\n",
    "    urls = []\n",
    "    with open(destFile) as f:\n",
    "        for row in f:\n",
    "            lists = row.split(\",\")\n",
    "            url = lists[5]\n",
    "            url = url.split(\";\")\n",
    "            for u in url:\n",
    "                urls.append(u)\n",
    "        urls.pop(0)\n",
    "        \n",
    "    urls =  list(dict.fromkeys(urls))\n",
    "    return urls\n",
    "\n",
    "def save_inf_urls():\n",
    "    cnt = 0\n",
    "    infringing_urls = set()\n",
    "    saved_urls = set()\n",
    "    if exists(\"infringing.pkl\"):\n",
    "        saved_urls = pickle.load(open('infringing.pkl','rb'))\n",
    "\n",
    "    with open(destFile) as f:\n",
    "        next(f)\n",
    "        for row in f:\n",
    "            lists = row.split(',')\n",
    "            lists = lists[5].split(\";\")\n",
    "            \n",
    "            for u in lists:\n",
    "                infringing_urls.add(u)\n",
    "\n",
    "        infringing_urls = infringing_urls.difference(saved_urls)\n",
    "\n",
    "    with open(infringing,'a') as f:\n",
    "        writer_obj = writer(f)\n",
    "        for u in infringing_urls:\n",
    "            writer_obj.writerow([u])\n",
    "            saved_urls.add(u)\n",
    "            cnt+=1\n",
    "    pickle.dump(saved_urls,open('infringing.pkl','wb'))\n",
    "    print(\"Saved \" + str(cnt) + \" Infringing Urls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74223192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_bad_word(url):\n",
    "    for u in bad_words:\n",
    "        if u in url:\n",
    "            return True,u\n",
    "    return False,None\n",
    "\n",
    "def get_infringing_urls():\n",
    "    urls = []\n",
    "    with open(destFile) as f:\n",
    "        for row in f:\n",
    "            lists = row.split(\",\")\n",
    "            url = lists[6]\n",
    "            url = url.split(\";\")\n",
    "            for u in url:\n",
    "                urls.append(u)\n",
    "        urls.pop(0)\n",
    "        \n",
    "    urls =  list(dict.fromkeys(urls))\n",
    "    return urls\n",
    "\n",
    "def save_inf_urls():\n",
    "    \n",
    "    cnt = 0\n",
    "    infringing_urls = set()\n",
    "    saved_urls = set()\n",
    "    \n",
    "    if exists(\"infringing.pkl\"):\n",
    "        saved_urls = pickle.load(open('infringing.pkl','rb'))\n",
    "\n",
    "    with open(destFile) as f:\n",
    "        next(f)\n",
    "        for row in f:\n",
    "            lists = row.split(',')\n",
    "            lists = lists[5].split(\";\")\n",
    "            \n",
    "            for u in lists:\n",
    "                infringing_urls.add(u)\n",
    "\n",
    "        infringing_urls = infringing_urls.difference(saved_urls)\n",
    "\n",
    "    with open(infringing,'a') as f:\n",
    "        writer_obj = writer(f)\n",
    "        for u in infringing_urls:\n",
    "            writer_obj.writerow([u])\n",
    "            saved_urls.add(u)\n",
    "            cnt+=1\n",
    "    \n",
    "    pickle.dump(saved_urls,open('infringing.pkl','wb'))\n",
    "    print(\"Saved \" + str(cnt) + \" Infringing Urls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a2736",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    if GET_URLS == 4 and exists(destFile):\n",
    "        save_inf_urls()\n",
    "        exit()\n",
    "\n",
    "    read_url = set()\n",
    "    bad_urls = set()\n",
    "\n",
    "    # Load the list of urls already read\n",
    "    if exists(\"read_urls.pkl\"):\n",
    "        read_url = pickle.load(open(\"read_urls.pkl\",\"rb\"))\n",
    "\n",
    "    # Load the list of bad urls found\n",
    "    if exists(\"bad_urls.pkl\"):\n",
    "        bad_urls = pickle.load(open(\"bad_urls.pkl\",\"rb\"))\n",
    "    \n",
    "    # Mode selection\n",
    "    if not exists(destFile) or GET_URLS == 1:\n",
    "        urls = init_url\n",
    "    elif GET_URLS == 2:\n",
    "        urls = get_infringing_urls()\n",
    "    elif exists(\"bad_urls.pkl\") and GET_URLS == 3:\n",
    "        urls = bad_urls\n",
    "    else:\n",
    "        urls = init_url\n",
    "        \n",
    "    # Counters\n",
    "    cnt_inserted = 0\n",
    "    cnt_total = 0\n",
    "\n",
    "    # Check if destination file exits\n",
    "    destFExist = True\n",
    "    if not exists(destFile):\n",
    "        destFExist = False\n",
    "        bad_urls = set()\n",
    "        read_url = set()\n",
    "    \n",
    "    # If destFile already exists then load the id column in a list.\n",
    "    ids = []\n",
    "    if destFExist:\n",
    "        with open(destFile) as f:\n",
    "            for row in f:\n",
    "                lists = row.split(',')\n",
    "                ids.append(lists[0])\n",
    "            ids.pop(0)\n",
    "\n",
    "    # Convert the ids from string to uint32\n",
    "    ids = np.uint32(ids).tolist()\n",
    "\n",
    "    # The total number of row in the destFile starts from the initial number of row\n",
    "    cnt_total = len(ids)\n",
    "\n",
    "    # Open destFile. If not exist it creates it automatically\n",
    "    with open(destFile,'a', newline='') as f:\n",
    "        writer_obj = writer(f)\n",
    "        # If the file didn't exist initialize the field names row\n",
    "        if not destFExist:\n",
    "            writer_obj.writerow(fieldnames)\n",
    "        \n",
    "        # Subtract the already read urls from the complete list of urls.\n",
    "        # In this way I have just the unread urls.\n",
    "        # Add problematic urls manually to the list\n",
    "        read_url = set(read_url)\n",
    "        \n",
    "        read_url = read_url.union(set(to_skip))\n",
    "        urls = list(set(urls) - read_url)\n",
    "        for url in urls:\n",
    "            print(\"########\")\n",
    "            print(\"Start fetching data about \"+url)\n",
    "\n",
    "            page_num = 1\n",
    "\n",
    "            while True:\n",
    "\n",
    "                time.sleep(1)\n",
    "                \n",
    "                query = endpoint+typeSearch+\"=\"+url+\"&term-require-all=true\"+\"&\"+\"authentication_token=\"+APIkey\n",
    "                print(query)\n",
    "\n",
    "                response = requests.get(query+\"&page=\"+str(page_num), headers=headers)\n",
    "                print(response)\n",
    "\n",
    "                # Check the status code before processing the data\n",
    "                if response.status_code == 200:\n",
    "                    response_data = response.json()\n",
    "\n",
    "                    print(\"fetching relevant data\")\n",
    "                    for notice in response_data['notices']:\n",
    "                        works = notice.get(\"works\", [])\n",
    "                        for work in works:\n",
    "                            try:\n",
    "                                # Store just the urls with unseen id and type \"DMCA\"\n",
    "                                if (not notice.get(\"id\") in ids and notice.get(\"type\") == \"DMCA\" and notice.get(\"score\") >= 20):\n",
    "                                    \n",
    "\n",
    "                                    copyrighted_urls = work.get(\"copyrighted_urls\")\n",
    "                                    infringing_urls =  work.get(\"infringing_urls\")\n",
    "                                    \n",
    "                                    # If copywrighted_urls or infringing_urls is equal to None skip this iteration\n",
    "                                    if copyrighted_urls == None or infringing_urls == None:\n",
    "                                        continue\n",
    "\n",
    "                                    copyrighted = \"\"\n",
    "                                    infringing = \"\"\n",
    "                                    \n",
    "                                    # Create a string with all the copywrighted urls.\n",
    "                                    for i in range(0,len(copyrighted_urls)):\n",
    "                                        copyurl = copyrighted_urls[i].get(\"url\")\n",
    "                                        cbw,u = contains_bad_word(copyurl)\n",
    "                                        if not SAVE_ONLY_BAD_URLS or SAVE_ONLY_BAD_URLS and cbw:\n",
    "                                            copyrighted += \";\" + copyurl\n",
    "                                            # If a copywrighted url contains a bad word, store it in a list.                \n",
    "                                            splits = copyurl.split(\"/\")\n",
    "                                                \n",
    "                                            if u == \"tumblr\":\n",
    "                                                if len(splits) >= 4:\n",
    "                                                    if splits[3] == \"blog\":\n",
    "                                                        copyurl = \"/\".join(splits[:6])\n",
    "                                                    elif splits[3] == \"post\":\n",
    "                                                        copyurl = \"/\".join(splits[:3])\n",
    "                                                    else:\n",
    "                                                        copyurl = \"/\".join(splits[:4])\n",
    "                                            elif u == \"wordpress\" :\n",
    "                                                if splits[2] == \"wordpress.com\":\n",
    "                                                    copyurl = \"/\".join(splits[:len(splits)-1])\n",
    "                                                else:\n",
    "                                                    copyurl = \"/\".join(splits[:3])\n",
    "                                            \n",
    "                                            elif u == \"issuu\":\n",
    "                                                    copyurl = \"/\".join(splits[:4])\n",
    "                                            \n",
    "                                            else:\n",
    "                                                copyurl = \"/\".join(splits[:3])\n",
    "                                            if cbw:\n",
    "                                                bad_urls.add(copyurl)\n",
    "                                            break\n",
    "                                    \n",
    "                                    if copyrighted != \"\":\n",
    "                                        # Create a string with all the infringing urls.\n",
    "                                        for i in range(0,len(infringing_urls)):\n",
    "                                            infringing += \";\" + infringing_urls[i].get(\"url\")\n",
    "                                        \n",
    "                                        # Delete the first semicolon character\n",
    "                                        copyrighted = copyrighted[1:]\n",
    "                                        infringing = infringing[1:]\n",
    "\n",
    "                                        to_write = [notice.get(\"id\"),\n",
    "                                                    notice.get(\"type\"),\n",
    "                                                    notice.get(\"date_received\")[0:10],\n",
    "                                                    notice.get(\"sender_name\"),\n",
    "                                                    notice.get(\"jurisdictions\")[0].upper(),\n",
    "                                                    copyrighted,\n",
    "                                                    infringing,\n",
    "                                                    url,\n",
    "                                                    notice.get(\"score\")]\n",
    "                                        # Write the data to the CSV file without the header row\n",
    "                                        writer_obj.writerow(to_write)\n",
    "                                        # Store in the list the id\n",
    "                                        ids.append(notice.get(\"id\"))\n",
    "                                        # Increase the counter of inserted ids\n",
    "                                        cnt_inserted +=1\n",
    "                            except Exception as e:\n",
    "                                print(\"Error writing to CSV file:\", e)\n",
    "\n",
    "                read_url.add(url)\n",
    "                \n",
    "                # Store in .pkl files the read urls and the bad urls\n",
    "                pickle.dump(bad_urls,open('bad_urls.pkl','wb'))\n",
    "                pickle.dump(read_url,open('read_urls.pkl','wb'))\n",
    "\n",
    "                if len (response_data['notices']) == 0:\n",
    "                    break\n",
    "                page_num += 1\n",
    "        cnt_total+=cnt_inserted\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"Total number of rows: \" + str(cnt_total))\n",
    "    print(\"Inserted rows: \"+ str(cnt_inserted))\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
